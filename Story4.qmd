---
title: "Story 4: How much do we get paid?"
author: "Naomi Buell"
format: pptx
editor: visual
---

```{r}
#| label: load packages
#| message: false
library(tidyverse)
library(janitor)
library(jsonlite)
library(httr)
library(scales)
set.seed(123)
library(RColorBrewer)
```

## Intro

For this story we will answer the question, "How much do we get paid?"

My analysis and data visualizations address the variation in average salary for "Data Practitioners" based on role descriptor and state. The term "Data Practitioner" is a generic job descriptor which includes many different job role titles for individuals whose work activities overlap including Data Scientist, Data Engineer, Data Analyst, Business Analyst, Data Architect, etc.

## Get data from the Bureau of Labor Statistics (BLS) through the CareerOneStop API

Here, I download salary data from BLS Occupational Employment and Wages Statistics (OEWS) using the [CareerOneStop.org API](https://www.careeronestop.org/Developers/WebAPI/technical-information.aspx). I get salary data by state on [data scientists](https://www.bls.gov/ooh/math/data-scientists.htm), which include data analytics specialists, data mining analysts, data visualization developers, and business intelligence developers. I also pull data on [database administrators and architects](https://www.bls.gov/ooh/computer-and-information-technology/database-administrators.htm), which include automatic data processing planners, data architects, database administration managers, database coordinators, database developers, database programmers, database security administrators, data integration specialists, data warehousing specialists, and automatic data processing planners. Data is from May 2023.

```{r}
#| label: Pull data w/ CareerOneStop API
#| error: true
#| message: false
#| warning: false
#| results: hide

# API credentials
api_token <- "yewsVIARypHPbfHNt0GchJsjUeX3dbkJArnwCuvaUKklKZEssGpl9NuLO4jIeGJng+Q8raXwGDrcag6UsfDtlA=="
userid <- "1K9fclpWIkgV5X8"
base_url <- "https://api.bls.gov/publicAPI/v2/timeseries/data/"
locations <- state.abb
keywords <- c("152051", # Data scientists
              "151243", # Database architects
              "151242") # Database Administrators

# Initialize list and df for storage in the proceeding loop
data <- list()

# Loop through state and job codes and pull data w/ API
for (location in locations) {
  for (keyword in keywords) {
    request_url <- paste0(
      "https://api.careeronestop.org/v1/comparesalaries/",
      userid,
      "/wage?keyword=",
      keyword,
      "&location=",
      location,
      "&enableMetaData=false"
    )
    
    response <- httr::GET(
      url = request_url,
      httr::add_headers(
        "Content-Type" = "application/json",
        "Authorization" = paste("Bearer", api_token)
      ),
      query = list(
        keyword = keyword,
        location = location,
        enableMetaData = FALSE
      )
    )
    
    # Parse the JSON response
    data[[location]][[keyword]] <- httr::content(response, as = "parsed", type = "application/json")
    
    # Add a delay to avoid hitting the API rate limit
    Sys.sleep(0.4)
  }
}
```

I bind data together, keeping only variables of interest.

```{r}
#| label: bind and clean data

results <- tibble()

for (location in locations) {
  for (keyword in keywords) {
      title <- data[[location]][[keyword]][["OccupationDetail"]][["OccupationTitle"]]
      wages_list <- data[[location]][[keyword]][["OccupationDetail"]][["Wages"]][["StateWagesList"]]
      
      # Create a data frame from the StateWagesList
      wage_df <- do.call(rbind, lapply(wages_list, function(wage) {
        data.frame(
          median = wage[["Median"]],
          rate_type = wage[["RateType"]],
          pct_10 = wage[["Pct10"]],
          pct_25 = wage[["Pct25"]],
          pct_75 = wage[["Pct75"]],
          pct_90 = wage[["Pct90"]],
          stringsAsFactors = F
        )
      }))
      
      # Add the state and occupation title to the wage_df
      wage_df$state <- location
      wage_df$occupation_title <- title
      
      # Append to results data frame
      results <- rbind(results, wage_df)
  }
}
```

I clean and tidy the resulting data frame below.

```{r}
#| label: clean df

results_clean <- results |> 
  mutate(
    state = as_factor(state),
    occupation_title = as_factor(occupation_title),
    median = as.numeric(median),
    pct_10 = as.numeric(pct_10),
    pct_25 = as.numeric(pct_25),
    pct_75 = as.numeric(pct_75),
    pct_90 = as.numeric(pct_90)
  ) |> 
  clean_names() |> 
  select(state, occupation_title, rate_type, median, starts_with("pct")) |> 
  arrange(state, desc(median))

head(results_clean)
```

Get number employed data.

```{r}
#| label: get df

path <- "https://raw.githubusercontent.com/naomibuell/DATA608/refs/heads/main/state_M2023_dl.csv"
df <- read.csv(path) |>
  clean_names() |>
  mutate(across(
    c(tot_emp, jobs_1000, emp_prse, starts_with("a_")),
    ~ as.numeric(str_remove(., ","))
  ))

df |> head()
```

Note: EMP_PRSE = Percent relative standard error (PRSE) for the employment estimate. PRSE is a measure of sampling error, expressed as a percentage of the corresponding estimate. Sampling error occurs when values for a population are estimated from a sample survey of the population, rather than calculated from data for all members of the population. Estimates with lower PRSEs are typically more precise in the presence of sampling error.

## Graph data

In my visualizations below, I show the most salient information (variation in average salary by role and by state).

```{r}
#| label: data prep

df_levels <- df |> 
  mutate(state = prim_state) |> 
  group_by(state) |> 
  summarize(tot_emp = sum(tot_emp, na.rm = T),
            jobs_1000 = sum(jobs_1000, na.rm = T))

# Assign levels by total number employed
levels_tot_emp <- df_levels |> 
  filter(state %in% results_clean$state) |> 
  arrange(desc(tot_emp)) |> 
  pull(state)

# Assign levels by number employed/1000 (state population adjusted)
levels_jobs_1000 <- df_levels |> 
  filter(state %in% results_clean$state) |> 
  arrange(desc(jobs_1000)) |> 
  pull(state)

# Order occupations
occupation_order <- df |>
  group_by(occ_title) |>
  summarize(mean = mean(a_mean, na.rm = TRUE)) |>
  arrange(mean) |>
  pull(occ_title)
```

```{r}
#| label: redo pop adjusted heatmap by average salary instead of median.

# Assign levels by number employed/1000 (state population adjusted)
levels_jobs_1000 <- df_levels |> 
  arrange(desc(jobs_1000)) |> 
  pull(state)


df |>
  mutate(
    state = factor(prim_state, levels = levels_jobs_1000),
    occupation_title = factor(occ_title, levels = occupation_order)
  ) |>
  ggplot(aes(x = occupation_title, y = fct_rev(state), fill = a_mean)) +
  geom_tile(color = "white") +
  scale_fill_gradientn(colors = (brewer.pal(9, "Purples")), 
                       name = "Average salary",
                       labels = dollar_format()) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  labs(x = "", y = "States from largest to smallest population-adjusted data workforce")
```

```{r}
#| label: get range of average salaries

df |> select(a_mean) |> summary()

```

```{r}
#| label: get range of median salaries

results_clean |>
  filter(rate_type == "Annual") |> 
  select(median) |> 
  summary()
```

```{r}
#| label: prep data and levels for iqr charts

# Gen IQR
df_dist <- df |> 
  mutate(iqr = a_pct75 - a_pct25)

# Order occupations
occupation_order_iqr <- df_dist |>
  group_by(occ_title) |>
  summarize(mean = mean(iqr, na.rm = TRUE)) |>
  arrange(mean) |>
  pull(occ_title)

# Assign levels to states by iqr
levels_state_iqr <- df_dist |> 
  group_by(prim_state) |> 
  summarise(mean_iqr = mean(iqr)) |> 
  arrange(desc(mean_iqr)) |> 
  pull(prim_state)

```

```{r}
#| label: weight IQRs by tot employed, summarize by occupation. 

df_weighted_by_occ <- df_dist |>
  drop_na(tot_emp) |>
  uncount(tot_emp / 10) |>
  group_by(occ_title) |>
  summarise(
    weighted_avg_iqr = mean(iqr),
    weighted_avg_sal = mean(a_mean),
    weighted_median_sal = mean(a_median),
    weighted_avg_pct25 = mean(a_pct25),
    weighted_avg_pct75 = mean(a_pct75)
  )

df_weighted_by_occ |>  head()


df_weighted_by_occ |>
  ggplot(aes(x = occ_title, y = weighted_median_sal)) +
  geom_bar(stat = "identity", fill = brewer.pal(3, "Set3")[3]) +
  geom_errorbar(aes(ymin = weighted_avg_pct25, ymax = weighted_avg_pct75), width = 0.2) +
  scale_y_continuous(labels = dollar_format(
    scale = 1e-3,
    prefix = "$",
    suffix = "K"
  )) +
  theme_minimal() +
  labs(x = "", y = "Median salary") +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )
```

```{r}
#| label: get range of IQRs

df_dist |> 
  select(prim_state, iqr, occ_title) |> 
  summary()

```

IQRs range from \$3,970 (Mississippi data architects) to \$116,600 at widest (Kansas data architects).

```{r}
#| label: graph IQRs by state, sorted by IQR


# Gen weighted avg by state
df_weighted_by_state <- df_dist |>
  drop_na(tot_emp) |>
  uncount(tot_emp / 10) |>
  group_by(prim_state) |>
  summarise(
    weighted_avg_iqr = mean(iqr),
    weighted_avg_sal = mean(a_mean),
    weighted_median_sal = mean(a_median),
    weighted_avg_pct25 = mean(a_pct25),
    weighted_avg_pct75 = mean(a_pct75)
  )

# Graph with custom legend
df_weighted_by_state |>
  mutate(prim_state = factor(prim_state, levels = levels_state_iqr)) |>
  ggplot(aes(x = prim_state)) +
  
  # Mean point
  geom_point(aes(y = weighted_avg_sal, color = "Average"), size = 3) +
  
  # Median bars
  geom_bar(aes(y = weighted_median_sal, fill = "Median"), stat = "identity") +
  
  # IQR error bars
  geom_errorbar(aes(ymin = weighted_avg_pct25, ymax = weighted_avg_pct75, linetype = "IQR (25th-75th)"), width = 0.2, color = "black") +
  
  # Custom legend
  scale_fill_manual(name = "", values = c("Median" = brewer.pal(3, "Set3")[3])) +
  scale_color_manual(name = "", values = c("Average" = "black")) +
  scale_linetype_manual(name = "",
                        values = c("IQR (25th-75th)" = "solid")) +
  
  # Dollar format for y-axis
  scale_y_continuous(labels = dollar_format(scale = 1e-3, prefix = "$", suffix = "K")) +
  
  # Theme and labels
  theme_minimal() +
  labs(x = "States from largest spread to smallest", y = "Salary") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top")
  

```

```{r}
#| label: investigate WV

df_dist |> filter(prim_state == "WV") |> head()

```
